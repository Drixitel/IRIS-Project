{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Structure \n",
    " - t_Before, t_After, A0, A1  = 4 bytes, 4 bytes, 4 bytes, 4 bytes = 16 bytes\n",
    " - Header = 3 x 4 bytes values  = 12 bytes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "binary_file = True\n",
    "outside_file = False\n",
    "\n",
    "if binary_file == True:\n",
    "    \"\"\" Extract Binary information from our files \"\"\"\n",
    "    import struct\n",
    "\n",
    "    path = 'data/09020003.DAT'  # Update this to the actual path\n",
    "    header = [0, 0, 0]  # Initialize header variable as a list of 3 integers\n",
    "    arrays = [[], [], [], []]  # Initialize 4 arrays\n",
    "    # print(header)\n",
    "    with open(path, mode='rb') as file:\n",
    "        # Read the first 3 values aka 12 bytes into the header\n",
    "        header = list(struct.unpack('<3I', file.read(12)))\n",
    "\n",
    "        while True:\n",
    "            data = file.read(16)  # Reading 16 bytes at a time (4 bytes for each interval)\n",
    "            if not data:  # If data is empty\n",
    "                break\n",
    "\n",
    "            # Extract values using struct.unpack\n",
    "            values = struct.unpack('<4I', data)\n",
    "\n",
    "            # Extract values manually without using struct.unpack\n",
    "            # values = (\n",
    "            #     int.from_bytes(data[0:4], 'little'),\n",
    "            #     int.from_bytes(data[4:8], 'little'),\n",
    "            #     int.from_bytes(data[8:12], 'little'),\n",
    "            #     int.from_bytes(data[12:16], 'little')\n",
    "            # )\n",
    "\n",
    "            # Add values to respective arrays\n",
    "            arrays[0].append(values[0])\n",
    "            arrays[1].append(values[1])\n",
    "            arrays[2].append(values[2])\n",
    "            arrays[3].append(values[3])\n",
    "\n",
    "    print(\"Header:\", header)\n",
    "    print(\"time1:\", np.array(arrays[0][:5]))\n",
    "    print(\"time2:\", np.array(arrays[1][:5]))\n",
    "    print(\"A0:\", arrays[2][:5])\n",
    "    print(\"A1:\", arrays[3][:5])\n",
    "\n",
    "    t1 = np.array(arrays[0]) # time before data collection \n",
    "    t2 = np.array(arrays[1]) # time after data collection \n",
    "\n",
    "\n",
    "if outside_file == True: \n",
    "    from scripts.analysis_IRIS import analyze, quickLook, extract_params\n",
    "    # directory_path = \"../M02M4/data/M0_tAv/50_1500_20/05100001.TXT\"\n",
    "    # directory_path = \"../M02M4/data/M0_tAv/100_500_20/09060001.TXT\"\n",
    "    directory_path = \"../M02M4/data/M4_tAv/50_1500_20/05090001.TXT\"\n",
    "    # Apply analyze function and get the resulting dictionary\n",
    "    quickLook(directory_path, save_png=False, plot_all=True, plot_hist_sampletime=False)\n",
    "    t, t1, t2, v0, v1, samples_averaged, inter_sample, inter_average = extract_params(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "print(\"\\n\\tPARAMETERS\")\n",
    "if outside_file == False:\n",
    "    inter_sample = header[0]\n",
    "    inter_average = header[1]\n",
    "    samples_averaged = header[2]\n",
    "print('inter_sample (us):\\t\\t', inter_sample)\n",
    "print('inter_average (us):\\t\\t', inter_average)\n",
    "print('samples_averaged:\\t\\t', samples_averaged)\n",
    "\n",
    "# Expected time spent sampling \n",
    "print(\"\\n\\tTIME SPENT SAMPLING - T2-T1\")\n",
    "t_s_expected = samples_averaged*inter_sample + inter_average\n",
    "print('Expected - No overhead (us):\\t', t_s_expected)\n",
    "# Actual Time spent sampling \n",
    "t_s = t2 - t1 \n",
    "t_s_med = np.median(t_s) # 2000 us\n",
    "print(\"Actual - median (us):\\t\\t\", t_s_med)\n",
    "t_s_err =  abs(t_s_med - t_s_expected)/t_s_expected\n",
    "print(\"% err:\\t\\t\\t\\t\",t_s_err*100)\n",
    "\n",
    "# Expected gap \n",
    "print(\"\\n\\tMEDIAN GAP - BETWEEN LINES\")\n",
    "print(\"Expected - No overhead (us):\\t\", inter_average)\n",
    "# Actual Gaps \n",
    "t_g = t1[1:] - t2[:-1]\n",
    "t_g_med = np.median(t_g) # ~500 us\n",
    "print(\"Actual - median gap (us):\\t\", t_g_med)\n",
    "t_g_err = abs(t_g_med - inter_average)/inter_average\n",
    "print(\"% err:\\t\\t\\t\\t\", round(t_g_err*100,3))\n",
    "print(\"Smallest gap (us):\\t\\t\", min(t_g))\n",
    "print(\"Largest gap (ms):\\t\\t\", max(t_g/1e3))\n",
    "\n",
    "\n",
    "# Averaged Time\n",
    "t_mid = (t1+t2)/2\n",
    "# Voltage \n",
    "if outside_file == False: \n",
    "    d0 = np.array(arrays[2])\n",
    "    d1 = np.array(arrays[3])\n",
    "    v0 = (d0/samples_averaged)*(3.3/4096)\n",
    "    v1 = (d1/samples_averaged)*(3.3/4096)\n",
    "\n",
    "# Make a list of the time differences (gaps) between adjacent points:\n",
    "dt = t_mid - np.roll(t_mid,1)\n",
    "# skip 1st value\n",
    "dt = dt[1:]\n",
    "# Collect gaps larger than the - Actual Time Spent Sampling \n",
    "gap_qualifier = t_s_med*2\n",
    "long_gap = np.where(dt > gap_qualifier)[0]\n",
    "print('\\n\\tLARGE GAPS - TIME')\n",
    "print('Expecting # of gaps: \\t\\t', len(long_gap))\n",
    "print('Size is larger than median time...\\n\\t...spent sampling (us):\\t', gap_qualifier)\n",
    "\n",
    "# look at the index locations of large gaps\n",
    "# print('num, idx, gap size')\n",
    "for count, value in enumerate(long_gap):\n",
    "    # print(count, value ,dt[value])\n",
    "    0\n",
    "# Compute the number of lines per write and bytes sent\n",
    "dt_idx = long_gap - np.roll(long_gap,1)\n",
    "dt_idx = dt_idx[1:]\n",
    "print('Median gap size (us):\\t\\t', np.median(dt[long_gap]))\n",
    "print('Smallest large gap (us):\\t', np.min(dt[long_gap]))\n",
    "print('Largest large gap (us):\\t\\t', np.max(dt[long_gap]))\n",
    "\n",
    "print('\\n\\tLARGE GAPS - LINES AND BYTES')\n",
    "print('Median # of lines:\\t\\t', np.median(dt_idx))\n",
    "print('Median # of Bytes:\\t\\t', np.median(dt_idx)*16)\n",
    "print('Modulo of 512 Bytes:\\t\\t', round((np.median(dt_idx)*16)/512,2))\n",
    "# Expected Byte and line quantities \n",
    "print(\"\\n\\tEXPECTED BYTE AND LINE COUNT\")\n",
    "# Expected # of bytes per line written \n",
    "print('Expected # of Bytes per line:\\t\\t', 16)\n",
    "# Expected large gap\n",
    "print(\"Expected byte chunk due to SD write:\\t\", 512)\n",
    "print(\"Expected # of lines written per chunk:\\t\", 512/16)\n",
    "\n",
    "# Confirm times add up to the correct file length \n",
    "print('\\n\\tFILE LENGTH')\n",
    "# Actual file time\n",
    "t_mid = t_mid - t_mid[0]\n",
    "file_t_actual = t_mid[-1]\n",
    "print('Actual File length (s):\\t\\t',round(file_t_actual/1e6,3))\n",
    "# Calculated time based on median gaps found \n",
    "small_gap = np.where(dt < gap_qualifier)[0]\n",
    "file_t = len(dt[long_gap])*np.median(dt[long_gap]) + t_s_med*len(small_gap)\n",
    "print('Calculation (s):\\t\\t',round(file_t/1e6,3))\n",
    "print('\\t# of large gaps x median large gap + # of median gap x median gap')\n",
    "file_t_err = abs(file_t_actual - file_t)/file_t\n",
    "print('% err:\\t\\t\\t\\t', round(file_t_err*100,3))\n",
    "\n",
    "print('\\n\\tTIME BETWEEN LARGE GAPS')\n",
    "# Did: t[0], t[1]-(t[0+1]) , t[2] - (t[1]+1), ...\n",
    "dt_b4_write = []\n",
    "dt_b4_write.append(t_mid[long_gap[0]])\n",
    "for i in range(1,len(long_gap[1:])): \n",
    "    new_val = t_mid[long_gap[i]] - (t_mid[long_gap[i-1]+1])\n",
    "    dt_b4_write.append(new_val)\n",
    "dt_b4_med = np.median(dt_b4_write)\n",
    "print(\"Actual time of data collection...\\n\\t... b4 write - median (us):\\t\", dt_b4_med/1e3)\n",
    "print(\"Max dt (us):\\t\\t\\t\\t\", np.max(dt_b4_write)/1e3)\n",
    "print(\"Min dt (us):\\t\\t\\t\\t\", np.min(dt_b4_write)/1e3)\n",
    "\n",
    "# Frequency from examining lines \n",
    "print('\\n\\tFREQUENCY')\n",
    "# Frequency of averaging \n",
    "freq2 = samples_averaged/t_s_med\n",
    "print('Samples Averaged Frequency (Hz):\\t\\t', round(freq2*1e6,2))\n",
    "print(f'\\tSamples Averaged {samples_averaged} / median time spent sampling {t_s_med} us')\n",
    "# Frequency of collection not including write gaps\n",
    "freq = np.median(dt_idx)/dt_b4_med\n",
    "print(\"\\nFrequency - not including write gaps (Hz):\\t\", round(freq*1e6,3))\n",
    "print(f'\\t{np.median(dt_idx)} lines / collection time {round(dt_b4_med/1e3,2)} ms ')\n",
    "\n",
    "expected_freq = len(t_mid) / file_t_actual\n",
    "print('\\nExpected freq (Hz):\\t\\t\\t\\t', round(expected_freq*1e6,3))\n",
    "print('\\t# of lines in file/ duration of file')\n",
    "\n",
    "# # Larger Views\n",
    "print('\\nNumber of lines between large gaps full view:')\n",
    "print(len(dt_idx),dt_idx)\n",
    "print('Large gaps full view:')\n",
    "print(len(dt[long_gap]), dt[long_gap])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of all gaps \n",
    "h,tax = np.histogram(dt,range=[0,max(dt)],bins=int(max(dt)/100.))\n",
    "# histogram of time spent sampling \n",
    "dt2 = t2-t1\n",
    "h2,tax2 = np.histogram(dt2,range=[0, max(dt2)], bins=int(max(dt2)/100.))\n",
    "\n",
    "fig, axs = plt.subplots(4)\n",
    "fig.set_size_inches(8,8)\n",
    "\n",
    "axs[0].scatter(t_mid,v0,s=4)\n",
    "axs[0].plot(t_mid,v0,alpha=0.5, label ='A0')\n",
    "axs[0].set_xlabel('Time (us)')\n",
    "axs[0].set_ylabel('Volts')\n",
    "axs[0].set_title('A0')\n",
    "# axs[0].legend()\n",
    "axs[0].grid()\n",
    "\n",
    "axs[1].scatter(t_mid,v1,s=4, color ='C1')\n",
    "axs[1].plot(t_mid,v1,alpha=0.5, color ='C1', label= 'A1')\n",
    "axs[1].set_xlabel('Time (us)')\n",
    "axs[1].set_ylabel('Volts')\n",
    "axs[1].set_title('A1')\n",
    "# axs[1].legend()\n",
    "axs[1].grid()\n",
    "\n",
    "#plot histogram of gaps in milliseconds:\n",
    "axs[2].plot(tax[1:]/1e3,h,alpha=0.5)\n",
    "axs[2].scatter(tax[1:]/1e3,h,s=4)\n",
    "axs[2].set_yscale('log')\n",
    "axs[2].set_xlabel('Gap (milliseconds)')\n",
    "axs[2].set_ylabel('Count')\n",
    "axs[2].set_title('Histogram of gaps')\n",
    "axs[2].grid()\n",
    "\n",
    "#plot histogram of gaps in milliseconds:\n",
    "axs[3].plot(tax2[1:]/1e3,h2,alpha=0.5)\n",
    "axs[3].scatter(tax2[1:]/1e3,h2,s=4)\n",
    "axs[3].set_xlabel('Sample Time (milliseconds)')\n",
    "axs[3].set_ylabel('Count')\n",
    "axs[3].set_title(f'Histogram of time spent sampling: Median time = {round(t_s_med/1e3,2)} ms')\n",
    "axs[3].grid()\n",
    "\n",
    "fig.suptitle(f'Samples Av:{samples_averaged}')\n",
    "fig.subplots_adjust(top=.93)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('plot2.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
