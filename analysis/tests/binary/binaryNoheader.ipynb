{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to view 1 file - No header\n",
    "\n",
    "## Memory Structure \n",
    " - t_Before, t_After, A0, A1  = 4 bytes, 4 bytes, 4 bytes, 4 bytes = 16 bytes\n",
    " - Header = 3 x 4 bytes values  = 12 bytes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7721\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "binary_file = True\n",
    "outside_file = False\n",
    "\n",
    "# get file memory size \n",
    "\n",
    "if binary_file == True:\n",
    "    \"\"\" Extract Binary information from our files \"\"\"\n",
    "    import struct\n",
    "\n",
    "    path = 'data/serial_data_20240510_124851_.dat'  # Update this to the actual path\n",
    "    header = [0, 0, 0]  # Initialize header variable as a list of 3 integers\n",
    "    arrays = [[], [], [], []]  # Initialize 4 arrays\n",
    "    # print(header)\n",
    "    with open(path, mode='rb') as file:\n",
    "        # Read the first 3 values aka 12 bytes into the header\n",
    "        # header = list(struct.unpack('<3I', file.read(12)))\n",
    "        # header = list(struct.unpack('<2I', file.read(8)))\n",
    "\n",
    "        while True:\n",
    "            data = file.read(4)  # Reading 16 bytes at a time (4 bytes for each interval)\n",
    "            if not data:  # If data is empty\n",
    "                break\n",
    "\n",
    "            # Extract values using struct.unpack\n",
    "            values = struct.unpack('<I', data)\n",
    "\n",
    "            # Extract values manually without using struct.unpack\n",
    "            # values = (\n",
    "            #     int.from_bytes(data[0:4], 'little'),\n",
    "            #     int.from_bytes(data[4:8], 'little'),\n",
    "            #     int.from_bytes(data[8:12], 'little'),\n",
    "            #     int.from_bytes(data[12:16], 'little')\n",
    "            # )\n",
    "\n",
    "            # Add values to respective arrays\n",
    "            # arrays[0].append(values[0])\n",
    "            # arrays[1].append(values[1])\n",
    "            # arrays[2].append(values[2])\n",
    "            # arrays[3].append(values[3])\n",
    "\n",
    "    # print(\"Header:\", header)\n",
    "    # print(\"time1:\", np.array(arrays[0][:5]))\n",
    "    # print(\"time2:\", np.array(arrays[1][:5]))\n",
    "    # print(\"BOF:\", np.array(arrays[0][0]))\n",
    "    # print(\"EOF:\", np.array(arrays[1][-1]))\n",
    "    # print(\"A0:\", arrays[2][:5])\n",
    "    # print(\"A1:\", arrays[3][:5])\n",
    "    # print(\"num of points:\", len(arrays[0]))\n",
    "\n",
    "    t1 = np.array(arrays[0]) # time before data collection \n",
    "    t2 = np.array(arrays[1]) # time after data collection \n",
    "\n",
    "i = 0\n",
    "print(values[i])\n",
    "# print(values[i+1])\n",
    "# print(values[i+2])\n",
    "# print(values[i+3])\n",
    "\n",
    "# print(arrays[i][:5])\n",
    "# print(arrays[i+1][:5])\n",
    "# print(arrays[i+2][:5])\n",
    "# print(arrays[i+3][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tPARAMETERS\n",
      "inter_sample (us):\t\t 0\n",
      "inter_average (us):\t\t 0\n",
      "samples_averaged:\t\t 0\n",
      "\n",
      "\tTIME SPENT SAMPLING - T2-T1\n",
      "Expected - No overhead (us):\t 0\n",
      "\t( samples_av:0 x inter_sample:0 ) + inter_av:0\n",
      "Actual - median (us):\t\t 0.0\n",
      "diff (us):\t\t\t 0.0\n",
      "\n",
      "\tMEDIAN GAP - BETWEEN AV. POINTS - T1-T2\n",
      "Expected - inter_average (us):\t 0\n",
      "Actual - median gap (us):\t 0.0\n",
      "Smallest gap (us):\t\t -23\n",
      "Largest gap (ms):\t\t 0.03\n",
      "\n",
      "\tLARGE GAPS - TIME\n",
      "Expecting # of gaps: \t\t 622\n",
      "Large gap >= Qualifier (us):\t 0.0\n",
      "\n",
      "Median Gap (us): \t\t 0.0\n",
      "Median LARGE gap size (us):\t 3.5\n",
      "Smallest Gap (us):\t\t -18.5\n",
      "Smallest LARGE gap (us):\t 0.5\n",
      "Largest Gap (us):\t\t 21.0\n",
      "Largest LARGE gap (us):\t\t 21.0\n",
      "\n",
      "\tLARGE GAPS - LINES AND BYTES\n",
      "Median # of lines:\t\t 2.0\n",
      "Median # of Bytes:\t\t 32.0\n",
      "Modulo of 512 Bytes:\t\t 0.06\n",
      "\n",
      "\tEXPECTED BYTE AND LINE COUNT\n",
      "Expected # of Bytes per line:\t\t 16\n",
      "Expected byte chunk due to SD write:\t 512\n",
      "Expected # of lines written per chunk:\t 32.0\n",
      "\n",
      "\tFILE LENGTH\n",
      "Actual File length (s):\t\t -0.0\n",
      "Calculation (s):\t\t 0.002\n",
      "\t# of large gaps x median large gap + # of median gap x median gap\n",
      "% err:\t\t\t\t 100.23\n",
      "\n",
      "\tTIME BETWEEN LARGE GAPS\n",
      "Actual time of data collection...\n",
      "\t... b4 write - median (ms):\t -0.004\n",
      "Min dt (ms):\t\t\t\t -0.0205\n",
      "Max dt (ms):\t\t\t\t 0.0\n",
      "\n",
      "\tFREQUENCY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mpmun\\AppData\\Local\\Temp\\ipykernel_22164\\1236589792.py:46: RuntimeWarning: divide by zero encountered in divide\n",
      "  v0 = (d0/samples_averaged)*(3.3/4096)\n",
      "C:\\Users\\mpmun\\AppData\\Local\\Temp\\ipykernel_22164\\1236589792.py:47: RuntimeWarning: divide by zero encountered in divide\n",
      "  v1 = (d1/samples_averaged)*(3.3/4096)\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 130\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mFREQUENCY\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#---------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Frequency of averaging \u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Expected Frequency (Hz)\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m expected_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m1e6\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt_s_expected\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected Freq:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m(expected_freq,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    132\u001b[0m freq2 \u001b[38;5;241m=\u001b[39m samples_averaged\u001b[38;5;241m/\u001b[39mt_s_med\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "# Parameters \n",
    "print(\"\\n\\tPARAMETERS\") #------------------------------------------------------------------------\n",
    "if outside_file == False:\n",
    "    inter_sample = header[0]\n",
    "    inter_average = header[1]\n",
    "    samples_averaged = header[2]\n",
    "print('inter_sample (us):\\t\\t', inter_sample)\n",
    "print('inter_average (us):\\t\\t', inter_average)\n",
    "print('samples_averaged:\\t\\t', samples_averaged)\n",
    "\n",
    "# Expected time spent sampling : T1 Intra T2 Inter T1 \n",
    "print(\"\\n\\tTIME SPENT SAMPLING - T2-T1\") #------------------------------------------------------------------------\n",
    "t_s_expected = samples_averaged*inter_sample + inter_average\n",
    "print('Expected - No overhead (us):\\t', t_s_expected)\n",
    "print(f'\\t( samples_av:{samples_averaged} x inter_sample:{inter_sample} ) + inter_av:{inter_average}')\n",
    "# Actual Time spent sampling \n",
    "t_s = t2 - t1 \n",
    "t_s_med = np.median(t_s) # 2000 us\n",
    "print(\"Actual - median (us):\\t\\t\", t_s_med)\n",
    "t_s_diff = t_s_med - t_s_expected\n",
    "\n",
    "print(\"diff (us):\\t\\t\\t\",round(t_s_diff,3))\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\\tMEDIAN GAP - BETWEEN AV. POINTS - T1-T2\") #-----------------------------------------------------------\n",
    "# Expected gap \n",
    "print(\"Expected - inter_average (us):\\t\", inter_average)\n",
    "# Actual Gaps \n",
    "t_g = t1[1:] - t2[:-1]\n",
    "t_g_med = np.median(t_g) # ~500 us\n",
    "print(\"Actual - median gap (us):\\t\", t_g_med)\n",
    "# t_g_err = abs(t_g_med - inter_average)/inter_average\n",
    "# print(\"% err:\\t\\t\\t\\t\", round(t_g_err*100,3))\n",
    "print(\"Smallest gap (us):\\t\\t\", min(t_g))\n",
    "print(\"Largest gap (ms):\\t\\t\", max(t_g/1e3))\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "print('\\n\\tLARGE GAPS - TIME') #---------------------------------------------------------------------------------\n",
    "# Averaged Time\n",
    "t_mid = (t1+t2)/2\n",
    "# Voltage \n",
    "if outside_file == False: \n",
    "    d0 = np.array(arrays[2])\n",
    "    d1 = np.array(arrays[3])\n",
    "    v0 = (d0/samples_averaged)*(3.3/4096)\n",
    "    v1 = (d1/samples_averaged)*(3.3/4096)\n",
    "\n",
    "# Make a list of the time differences (gaps) between adjacent points:\n",
    "dt = t_mid - np.roll(t_mid,1)\n",
    "# skip 1st value\n",
    "dt = dt[1:]\n",
    "# Collect gaps larger than the - Actual Time Spent Sampling \n",
    "gap_qualifier = t_s_med*1.3\n",
    "long_gap = np.where(dt > gap_qualifier)[0]\n",
    "print('Expecting # of gaps: \\t\\t', len(long_gap))\n",
    "print('Large gap >= Qualifier (us):\\t', gap_qualifier)\n",
    "\n",
    "print('')\n",
    "print('Median Gap (us): \\t\\t', np.median(dt))\n",
    "print('Median LARGE gap size (us):\\t', np.median(dt[long_gap]))\n",
    "print('Smallest Gap (us):\\t\\t', np.min(dt))\n",
    "print('Smallest LARGE gap (us):\\t', np.min(dt[long_gap]))\n",
    "print('Largest Gap (us):\\t\\t', np.max(dt))\n",
    "print('Largest LARGE gap (us):\\t\\t', np.max(dt[long_gap]))\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print('\\n\\tLARGE GAPS - LINES AND BYTES') #-----------------------------------------------------------------------\n",
    "# look at the index locations of large gaps\n",
    "# print('num, idx, gap size')\n",
    "for count, value in enumerate(long_gap):\n",
    "    # print(count, value ,dt[value])\n",
    "    0\n",
    "# Compute the number of lines per write and bytes sent\n",
    "dt_idx = long_gap - np.roll(long_gap,1)\n",
    "dt_idx = dt_idx[1:]\n",
    "print('Median # of lines:\\t\\t', np.median(dt_idx))\n",
    "print('Median # of Bytes:\\t\\t', np.median(dt_idx)*16)\n",
    "print('Modulo of 512 Bytes:\\t\\t', round((np.median(dt_idx)*16)/512,2))\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\\tEXPECTED BYTE AND LINE COUNT\")#-----------------------------------------------------------------------\n",
    "# Expected # of bytes per line written \n",
    "print('Expected # of Bytes per line:\\t\\t', 16)\n",
    "# Expected large gap\n",
    "print(\"Expected byte chunk due to SD write:\\t\", 512)\n",
    "print(\"Expected # of lines written per chunk:\\t\", 512/16)\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print('\\n\\tFILE LENGTH') #---------------------------------------------------------------------------------------       \n",
    "# Confirm times add up to the correct file length \n",
    "# Actual file time\n",
    "file_t_actual = t_mid[-1] - t_mid[0]\n",
    "print('Actual File length (s):\\t\\t',round(file_t_actual/1e6,3))\n",
    "# Calculated time based on median gaps found \n",
    "small_gap = np.where(dt < gap_qualifier)[0]\n",
    "file_t = len(dt[long_gap])*np.median(dt[long_gap]) + t_s_med*len(small_gap)\n",
    "print('Calculation (s):\\t\\t',round(file_t/1e6,3))\n",
    "print('\\t# of large gaps x median large gap + # of median gap x median gap')\n",
    "file_t_err = abs(file_t_actual - file_t)/file_t\n",
    "print('% err:\\t\\t\\t\\t', round(file_t_err*100,3))\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print('\\n\\tTIME BETWEEN LARGE GAPS') #---------------------------------------------------------------------------\n",
    "# Did: t[0], t[1]-(t[0+1]) , t[2] - (t[1]+1), ...\n",
    "t_mid = t_mid - t_mid[0]\n",
    "dt_b4_write = []\n",
    "dt_b4_write.append(t_mid[long_gap[0]])\n",
    "for i in range(1,len(long_gap[1:])): \n",
    "    new_val = t_mid[long_gap[i]] - (t_mid[long_gap[i-1]+1])\n",
    "    # if new_val > 66_000: \n",
    "    #         print(t_mid[long_gap[i]],t_mid[long_gap[i-1] + 1 ])\n",
    "    #         print(new_val)\n",
    "    dt_b4_write.append(new_val)\n",
    "dt_b4_med = np.median(dt_b4_write)\n",
    "dt_b4_min = np.min(dt_b4_write)\n",
    "dt_b4_max = np.max(dt_b4_write)\n",
    "\n",
    "print(\"Actual time of data collection...\\n\\t... b4 write - median (ms):\\t\", dt_b4_med/1e3)\n",
    "# print(\"Min dt (ms):\\t\\t\\t\\t\", min(dt_b4_write)/1e3)\n",
    "# print(\"Max dt (ms):\\t\\t\\t\\t\", max(dt_b4_write)/1e3)\n",
    "print(\"Min dt (ms):\\t\\t\\t\\t\", dt_b4_min/1e3)\n",
    "print(\"Max dt (ms):\\t\\t\\t\\t\", dt_b4_max/1e3)\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Frequency from examining lines \n",
    "print('\\n\\tFREQUENCY') #---------------------------------------------------------------------------------------\n",
    "# Frequency of averaging \n",
    "# Expected Frequency (Hz)\n",
    "expected_freq = 1e6/(t_s_expected)\n",
    "print(\"Expected Freq:\\t\\t\\t\\t\\t\", round(expected_freq,1))\n",
    "freq2 = samples_averaged/t_s_med\n",
    "print('Samples Averaged Frequency (Hz):\\t\\t', round(freq2*1e6,2))\n",
    "print(f'\\tSamples Averaged {samples_averaged} / median time spent sampling {t_s_med} us')\n",
    "# Frequency of collection not including write gaps\n",
    "freq = np.median(dt_idx)/dt_b4_med\n",
    "print(\"\\nFrequency - not including write gaps (Hz):\\t\", round(freq*1e6,3))\n",
    "print(f'\\t{np.median(dt_idx)} lines / collection time {round(dt_b4_med/1e3,2)} ms ')\n",
    "\n",
    "expected_freq = len(t_mid) / file_t_actual\n",
    "print('\\nExpected freq (Hz):\\t\\t\\t\\t', round(expected_freq*1e6,3))\n",
    "print('\\t# of lines in file/ duration of file')\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print('\\n\\tDEAD TIME') #---------------------------------------------------------------------------------------\n",
    "dead_time = dt[dt>gap_qualifier]\n",
    "dead_time = np.sum(dead_time) # us\n",
    "dead_time_per = dead_time/file_t_actual \n",
    "print('Gap qualifier (us):\\t ', gap_qualifier)\n",
    "print('File len (s):\\t\\t ', round(file_t_actual/1e6,2))\n",
    "print('Dead time sum (s):\\t ', round(dead_time/1e6,3))\n",
    "print('Dead time %:\\t\\t ', round(dead_time_per*100,3))\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# # # Larger Views\n",
    "# print('\\nNumber of lines between large gaps full view:')\n",
    "# print(len(dt_idx),dt_idx)\n",
    "# print('Large gaps full view:')\n",
    "# print(len(dt[long_gap]), dt[long_gap])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`bins` must be positive, when an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# histogram of all gaps \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m h,tax \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m100.\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# histogram of time spent sampling \u001b[39;00m\n\u001b[0;32m      4\u001b[0m dt2 \u001b[38;5;241m=\u001b[39m t2\u001b[38;5;241m-\u001b[39mt1\n",
      "File \u001b[1;32mc:\\Users\\mpmun\\CodeUsersMpmun\\IRIS-Project\\analysis\\.venv\\lib\\site-packages\\numpy\\lib\\histograms.py:780\u001b[0m, in \u001b[0;36mhistogram\u001b[1;34m(a, bins, range, density, weights)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;124;03mCompute the histogram of a dataset.\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    776\u001b[0m \n\u001b[0;32m    777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    778\u001b[0m a, weights \u001b[38;5;241m=\u001b[39m _ravel_and_check_weights(a, weights)\n\u001b[1;32m--> 780\u001b[0m bin_edges, uniform_bins \u001b[38;5;241m=\u001b[39m \u001b[43m_get_bin_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;66;03m# Histogram is an integer or a float array depending on the weights.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mpmun\\CodeUsersMpmun\\IRIS-Project\\analysis\\.venv\\lib\\site-packages\\numpy\\lib\\histograms.py:424\u001b[0m, in \u001b[0;36m_get_bin_edges\u001b[1;34m(a, bins, range, weights)\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    422\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`bins` must be an integer, a string, or an array\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_equal_bins \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 424\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`bins` must be positive, when an integer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    426\u001b[0m     first_edge, last_edge \u001b[38;5;241m=\u001b[39m _get_outer_edges(a, \u001b[38;5;28mrange\u001b[39m)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(bins) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: `bins` must be positive, when an integer"
     ]
    }
   ],
   "source": [
    "# histogram of all gaps \n",
    "h,tax = np.histogram(dt,range=[0,max(dt)],bins=int(max(dt)/100.))\n",
    "# histogram of time spent sampling \n",
    "dt2 = t2-t1\n",
    "h2,tax2 = np.histogram(dt2,range=[0, max(dt2)], bins=int(max(dt2)/100.))\n",
    "\n",
    "fig, axs = plt.subplots(4)\n",
    "fig.set_size_inches(8,8)\n",
    "\n",
    "axs[0].scatter(t_mid,v0,s=4)\n",
    "axs[0].plot(t_mid,v0,alpha=0.5, label ='A0')\n",
    "axs[0].set_xlabel('Time (us)')\n",
    "axs[0].set_ylabel('Volts')\n",
    "axs[0].set_title('A0')\n",
    "# axs[0].legend()\n",
    "axs[0].grid()\n",
    "\n",
    "axs[1].scatter(t_mid,v1,s=4, color ='C1')\n",
    "axs[1].plot(t_mid,v1,alpha=0.5, color ='C1', label= 'A1')\n",
    "axs[1].set_xlabel('Time (us)')\n",
    "axs[1].set_ylabel('Volts')\n",
    "axs[1].set_title('A1')\n",
    "# axs[1].legend()\n",
    "axs[1].grid()\n",
    "\n",
    "#plot histogram of gaps in milliseconds:\n",
    "axs[2].plot(tax[1:]/1e3,h,alpha=0.5)\n",
    "axs[2].scatter(tax[1:]/1e3,h,s=4)\n",
    "axs[2].set_yscale('log')\n",
    "axs[2].set_xlabel('Gap (milliseconds)')\n",
    "axs[2].set_ylabel('Count')\n",
    "axs[2].set_title(f'Histogram of gaps, {len(long_gap)} long gaps, median: {round(np.median(dt[long_gap])/1e3,3)} ms')\n",
    "axs[2].grid()\n",
    "\n",
    "#plot histogram of gaps in milliseconds:\n",
    "axs[3].plot(tax2[1:]/1e3,h2,alpha=0.5)\n",
    "axs[3].scatter(tax2[1:]/1e3,h2,s=4)\n",
    "axs[3].set_xlabel('Sample Time (milliseconds)')\n",
    "axs[3].set_ylabel('Count')\n",
    "axs[3].set_title(f'Histogram of time spent sampling: Median time = {round(t_s_med/1e3,3)} +- 0.2 ms')\n",
    "axs[3].grid()\n",
    "\n",
    "fig.suptitle(f'{path}, Samples_Av:{samples_averaged}, Inter_S:{inter_sample}, Inter_Av:{inter_average}')\n",
    "fig.subplots_adjust(top=.93)\n",
    "fig.tight_layout()\n",
    "\n",
    "# plt.savefig('plot2.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
